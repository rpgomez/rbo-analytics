# Introduction
This package provides an analytic for hypothesis testing that 2 Recommender systems are functionally related. The package makes use of the RBO (Rank Biased Overlap) scoring function developed by [researchers](https://dl.acm.org/doi/10.1145/1852102.1852106) from the University of Melbourne.

# How to install the package
1. clone this repository
2. using either pip or preferably uv install the package into your python environment like so:
```bash
cd rbo-analytics

# using uv
uv pip install .

# or using pip
pip install .
```

# How to use the package

## Statistically testing the relationship between 2 ranked lists
If you have 2 ranked lists of length $K$ produced by recommenders A and B and recommender A  provides a probability distribution for its list, then you can perform hypothesis testing that the list generated by recommender B is related to the list generated by recommender A like so:

```python
import rbo_analytics

# decide desired p-value cutoff for determining the validity of the null hypothesis/alternative hypothesis
pcutoff = .01

num_simulations = int(100/pcutoff+0.5) # want enough monte carlo simulations for accuracy.

# K determines how many of the ranked entries of list_a are important. If all matter, then K = len(list_a)
p = 0.01**(1/K) # How we determine the persistence parameter for RBO scores.

# perform simulations. probs is a sorted-in-descending-order list of
# probabilities for the ranked list provided by recommender A.
montecarlo_scores = rbo_analytics.perform_montecarlo(probs,K = K,T=num_simulations,verbose=True)

cutoff_value = montecarlo_scores[100] # This is the bottom %pcutoff of the generated scores.

rboscore = rbo_analytics.compute_rbo_score(list_a, list_b,p)

# Null hypothesis: the 2 ranked lists **are** related.
null_hypothesis_is_true = (rboscore > cutoff_value)
```
## Comparing document embedding models

* We have 2 different document embedding models, A & B. 
* We have a corpus of $N \gg 1$ documents. 

We would like to know if the 2 embedding models are statistically functionally related.

### Solution: 

1. For each document $D_n$ compute its corresponding vector embedding $E_n := A(D_n)$, $F_n = B(D_n)$.
2. For embedding model $A$ determine an appropriate value of $K$ as follows:
   1. Randomly select  $M \gg 1$ documents $\{D_m\}$ from the corpus. Let $\{E_m\}$ denote their corresponding vector embeddings.
   2. For each vector $E_m$ find its $P \ge 100$ closest neighbors $\{E_{m,p}\}$. **Exclude** $E_m$ from the list of neighbors. Compute the cosine similarity between vector $E_m$ and its $P$ closest neighors: $(<E_m,E_{m,p}>)$.
   3. Sort the cosine similarities as an ascending sequence $(cos_p)$.
   4. Find the elbow in the sequence of cosines $(cos_p)$ using an F-test to split the sequence into 2 subsequences. The size of the second subsequence is the corresponding value of $K_m$.
   5. We can either take $K= \max(\{K_m\})$ or keep the set of $K$ values $\{K_m\}$.
3. Now that we have a value of $K$ for each document $D_m$ we have a corresponding persistence parameter $p=0.01^{(1/K)}$.
4. For each document $D_m$ we have a ranked list of $K$ documents $list\_a = (i_1,\ldots,i_K)$ where $i_k$ denotes the index of the document that is $k^{th}$ closes to $D_m$   according to model $A$. We generate a similar second list of K ranked documents $list\_b = (j_1,\ldots, j_K)$ by proximity according to model $B$ (look at the corresponding vectors $(F_n)$.) For a corresponding probality distribution for model A's ranked list, we use the geometric based sequence 
$$prob = \frac{(1-p)}{1-p^{K}}\times (1,p,p**2, \ldots, p_{K-1})$$
5. Now we can use the rbo_analytics compute_recommender_test_statistic function to perform hypothesis testing.
```python
import rbo_analytics
Z = rbo_analytics.compute_recommender_test_statistic((lists_a, lists_b,probs,verbose=True)

print("Sigmage that the 2 document embedders are functionally related: {Z}")
print(f"The 2 document embedders are functionally related: {Z>=-2.33}")
```

## Comparing Nonlinear Projection Algorithms
One of the first things I wanted to do after developing a hypothesis testing analytic with RBO was to
compare the performance of my favorite local structure preserving nonlinear projection algorithm [UMAP](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Uniform_manifold_approximation_and_projection) and an
alternative algorithm [PACMAP](https://github.com/YingfanWang/PaCMAP).

The idea is similar to how one can compare document embedder maps:

* Take a high dimensional data set $D\subset \mathbb{R}^P$ with $P \gg 1$.
* Select a target dimension $Q \ll P$ to project down to.
* Project the data down to $X = UMAP(D) \subset \mathbb{R}^Q$ and $Y = PACMAP(D) \subset \mathbb{R}^Q$
* Just as in the document embedder scenario randomly select $M$ data points $\{D_m\}$ and look at the corresponding neighborhoods for $\{X_m\}$ anb $\{Y_m\}$.
* For comparing local structure preserving algorithms,
  * use the Euclidean metric to find and rank sort the $K$ nearest neighbors to each poin $X_m$ (respectively $Y_m$).
  * To determine an appropriate $K$ for each $X_m$ you can use the distribution of sorted *ascending* distances from each projected point to $X_m$ (exclude $X_m$ itself) and find the elbow in the plot. The length of the left segment is the value of $K$ to use. 
  * For each ranked list of $K$ points use the geometric probability distribution as in the document embedder scenario.
  * Now that you have lists of lists for both maps and the corresponding probability distribution for each list, you can use the compute_recommender_test_statistic function to perform hypothesis testing.
```python
import rbo_analytics
Z = rbo_analytics.compute_recommender_test_statistic((lists_a, lists_b,probs,verbose=True)

print("Sigmage that the 2 nonlinear projection algorithms are functionally related when it comes to preserving local structure: {Z}")
print(f"The 2 nonlinear projection algorithms are functionally related when it comes to preserving local structure: {Z>=-2.33}")
```

* For comparing their global structure preserving behavior, we invert the Euclidean metric: $||a - b|| \rightarrow 1/||a-b||$ to reoder the rankings and proceed as in the local structure preserving analysis.

## Comparing LLMs

### Testing for similarity in generative capabilities
We have 2 LLMs, LLM A and LLM B. I would like to know if they function similarly given the same prompts. My algorithm for determination is

* Construct/Obtain a set of $M$ prompt phrases (i.e. "Jack and Jill went up the", "Roses are red, violets are", "We the people")
* Feed each phrase into the corresponding LLM and obtain the logits $\{(token_i,logit_i)\}for the next token to be generated by each LLM.
* Notice that the logits output for the next token for a phrase is a *ranked* **recommended** list by the LLM, i.e. we can treat LLMs as recommender systems for tokens.
* For each phrase we can determine an appropriate value of $K$ by
   * apply the softmax onto the logits to get a probability distribution for tokens.
   * plot the sorted descending list of probabilities and use the elbow test to find an appropriate value of $K$ by the length of the left subsequence in the split of sorted values.
* For each phrase we have a ranked list of tokens from LLM A, $(list_{a,m})$ and one from LLM B $(list_{b,m})$ with a corresponding probs lists from LLM A. We can then apply the compute_recommender_test_statistic function
from rbo_analytics to determine if LLM A and B are related in their generative capacity.

**Problem**: Token generation is dependent on the tokenizer of the LLM. There is no standardized tokenizer used by distinct LLMs. That means that when comparing ranked lists it would be like comparing appples and oranges
which have very little in common.

**Resolution**: We infer a map between the 2 tokenizers in order to make comparisons of outputs. Our technique:

1. Let $vocab_A$ and $vocab_B$ denote the vocabularies of the respective tokenizers.
   1. For each tokenizer we determine if there exists a special token to denote the beginning of a token stream by passing a sequence of texts into the tokenizer and observe if the first encoded token is always the same in every generated token stream.
   2. For each tokenizer in order to detect the existence of the symbol indicating a new word we perform a frequency count $\{c_n\}$ of the first character in each token.  Which ever nonalphanumeric character has the highest count is probably the indicator of a new word beginning.
2. Let $tok_a$ and $tok_b$ denote tokens from the vocabularies of the tokenizers for LLM A and B respectively. Taking into account the possible
existence of a begin-of-stream token and a possible begin-word-character symbol we deduce a map from $vocab_A$ and $vocab_B$ as follows:
   1. Since the RBO analytic as we apply it is not symmetric in its arguments (the $K$ and $p$ parameters are determined by $A$, **not** $B$) we're only interested in the map $f:vocab_B \rightarrow vocab_A$ so we can compare the proposed tokens from LLM B to those from LLM A. For each pair of ranked lists we're comparing, we're only interested in mapping the top $K$ entries from each list.
> For each entry  in the list $A$ and each entryIf $tok_a$ is a prefix for $tok_b$ or vice versa, then we have a correspondence $tok_a  \leftrightarrow tok_b$
It's possible that more than 1 token from $vocab_A$ might map to the same token from $vocab_B$ and vice versa. If so we have 2 remediation strategies:
   * If we have 2 or more tokens $\{p_t\}$ mapping to a token $q$ from the other vocabulary, we assign the token $p_{t'}$ that is closest in edit distance to $q$.
   * If we have a token $q$ that maps to $p_1, p_2$ which are inequivalent

