# Introduction
This package provides an analytic for hypothesis testing that 2 Recommender systems are functionally related. The package makes use of the RBO (Rank Biased Overlap) scoring function developed by [researchers](https://dl.acm.org/doi/10.1145/1852102.1852106) from the University of Melbourne.

# How to install the package
1. clone this repository
2. using either pip or preferably uv install the package into your python environment like so:
```bash
cd rbo-analytics

# using uv
uv pip install .

# or using pip
pip install .
```

# How to use the package

## Statistically testing the relationship between 2 ranked lists
If you have 2 ranked lists of length $K$ produced by recommenders A and B and recommender A  provides a probability distribution for its list, then you can perform hypothesis testing that the list generated by recommender B is related to the list generated by recommender A like so:

```python
import rbo_analytics

# decide desired p-value cutoff for determining the validity of the null hypothesis/alternative hypothesis
pcutoff = .01

num_simulations = int(100/pcutoff+0.5) # want enough monte carlo simulations for accuracy.

# K determines how many of the ranked entries of list_a are important. If all matter, then K = len(list_a)
p = 0.01**(1/K) # How we determine the persistence parameter for RBO scores.

# perform simulations. probs is a sorted-in-descending-order list of
# probabilities for the ranked list provided by recommender A.
montecarlo_scores = rbo_analytics.perform_montecarlo(probs,K = K,T=num_simulations,verbose=True)

cutoff_value = montecarlo_scores[100] # This is the bottom %pcutoff of the generated scores.

rboscore = rbo_analytics.compute_rbo_score(list_a, list_b,p)

# Null hypothesis: the 2 ranked lists **are** related.
null_hypothesis_is_true = (rboscore > cutoff_value)
```
## Comparing document embedding models

* We have 2 different document embedding models: $A$ and $B$. 
* We have a corpus of $N \gg 1$ documents. 

We would like to know if the 2 embedding models are statistically functionally related.

### Solution: 

1. For each document $D_n$ compute its corresponding vector embedding $E_n := A(D_n)$, $F_n = B(D_n)$.
2. For embedding model $A$ determine an appropriate value of $K$ as follows:
   1. Randomly select  $M \gg 1$ documents $\{D_m\}$ from the corpus. Let $\{E_m\}$ denote their corresponding vector embeddings.
   2. For each vector $E_m$ find its $P \ge 100$ closest neighbors $\{E_{m,p}\}$. **Exclude** $E_m$ from the list of neighbors. Compute the cosine similarity between vector $E_m$ and its $P$ closest neighors: $(<E_m,E_{m,p}>)$.
   3. Sort the cosine similarities as an ascending sequence $(cos_p)$.
   4. Find the elbow in the sequence of cosines $(cos_p)$ using an F-test to split the sequence into 2 subsequences. The size of the second subsequence is the corresponding value of $K_m$.
   5. We can either take $K= \max(\{K_m\})$ or keep the set of $K$ values $\{K_m\}$.
3. Now that we have a value of $K$ for each document $D_m$ we have a corresponding persistence parameter $p=0.01^{(1/K)}$.
4. For each document $D_m$ we have a ranked list of $K$ documents $list\_a = (i_1,\ldots,i_K)$ where $i_k$ denotes the index of the document that is $k^{th}$ closes to $D_m$   according to model $A$. We generate a similar second list of K ranked documents $list\_b = (j_1,\ldots, j_K)$ by proximity according to model $B$ (look at the corresponding vectors $(F_n)$.) For a corresponding probality distribution for model A's ranked list, we use the geometric based sequence 
$$prob = \frac{(1-p)}{1-p^{K}}\times (1,p,p^2, \ldots, p_{K-1})$$
5. Now we can use the rbo_analytics compute_recommender_test_statistic function to perform hypothesis testing.
```python
import rbo_analytics

# lists_a is a list of ranked lists of neighbors for (D_m) according to recommender A.
# lists_b is a list of ranked lists of neighbors for (D_m) according to recommender B.
# probs is a list of list of probabilities, probs[m]  are the probabilities corresponding
# to a list of ranked neighbors lists_a[m] for document m according to A. 
Z = rbo_analytics.compute_recommender_test_statistic(lists_a, lists_b,probs,verbose=True)


print("Sigmage that the 2 document embedders are functionally related: {Z}")
print(f"The 2 document embedders are functionally related: {Z>=-2.33}")
```

## Comparing Nonlinear Projection Algorithms
One of the first things I wanted to do after developing a hypothesis testing analytic with RBO was to
compare the performance of my favorite local structure preserving nonlinear projection algorithm [UMAP](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Uniform_manifold_approximation_and_projection) and an
alternative algorithm [PACMAP](https://github.com/YingfanWang/PaCMAP).

The idea is similar to how one can compare document embedder maps:

* Take a high dimensional data set $D\subset \mathbb{R}^P$ with $P \gg 1$.
* Select a target dimension $Q \ll P$ to project down to.
* Project the data down to $X = UMAP(D) \subset \mathbb{R}^Q$ and $Y = PACMAP(D) \subset \mathbb{R}^Q$
* Just as in the document embedder scenario randomly select $M$ data points $\{D_m\}$ and look at the corresponding neighborhoods for $\{X_m\}$ anb $\{Y_m\}$.
* For comparing local structure preserving algorithms,
  * use the Euclidean metric to find and rank sort the $K$ nearest neighbors to each poin $X_m$ (respectively $Y_m$).
  * To determine an appropriate $K$ for each $X_m$ you can use the distribution of sorted *ascending* distances from each projected point to $X_m$ (exclude $X_m$ itself) and find the elbow in the plot. The length of the left segment is the value of $K$ to use. 
  * For each ranked list of $K$ points use the geometric probability distribution as in the document embedder scenario.
  * Now that you have lists of lists for both maps and the corresponding probability distribution for each list, you can use the compute_recommender_test_statistic function to perform hypothesis testing.
```python
import rbo_analytics
Z = rbo_analytics.compute_recommender_test_statistic(lists_a, lists_b,probs,verbose=True)

print("Sigmage that the 2 nonlinear projection algorithms are functionally related when it comes to preserving local structure: {Z}")
print(f"The 2 nonlinear projection algorithms are functionally related when it comes to preserving local structure: {Z>=-2.33}")
```

* For comparing their global structure preserving behavior, we invert the Euclidean metric: $||a - b|| \rightarrow 1/||a-b||$ to reoder the rankings and proceed as in the local structure preserving analysis.

## Comparing LLMs

### Testing for similarity in generative capabilities
We have 2 LLMs, LLM A and LLM B. I would like to know if they function similarly given the same prompts. My algorithm for determination is

* Construct/Obtain a set of $M$ prompt phrases (i.e. "Jack and Jill went up the", "Roses are red, violets are", "We the people")
* Feed each phrase into the corresponding LLM and obtain the logits $\{(token_i,logit_i)\}for the next token to be generated by each LLM.
* Notice that the logits output for the next token for a phrase is a *ranked* **recommended** list by the LLM, i.e. we can treat LLMs as recommender systems for tokens.
* For each phrase we can determine an appropriate value of $K$ by
   * apply the softmax onto the logits to get a probability distribution for tokens.
   * plot the sorted descending list of probabilities and use the elbow test to find an appropriate value of $K$ by the length of the left subsequence in the split of sorted values.
* For each phrase we have a ranked list of tokens from LLM A, $(list_{a,m})$ and one from LLM B $(list_{b,m})$ with a corresponding probs lists from LLM A. We can then apply the compute_recommender_test_statistic function
from rbo_analytics to determine if LLM A and B are related in their generative capacity.

**Problem**: Token generation is dependent on the tokenizer of the LLM. There is no standardized tokenizer used by distinct LLMs. That means that when comparing ranked lists it would be like comparing appples and oranges
which have very little in common.

**Resolution**: We infer a map between the 2 tokenizers in order to make comparisons of outputs. Our technique:

1. Let $vocab_A$ and $vocab_B$ denote the vocabularies of the respective tokenizers.

   1. For each tokenizer we determine if there exists a special token
   to denote the beginning of a token stream by passing a sequence of
   texts into the tokenizer and observe if the first encoded token is
   always the same in every generated token stream.
   
   2. For each tokenizer in order to detect the existence of the
   symbol indicating a new word we perform a frequency count $\{c_n\}$
   of the first character in each token.  Whichever nonalphanumeric
   character has the highest count is probably the indicator of a new
   word beginning.

2. Let $tok_a$ and $tok_b$ denote tokens from the vocabularies of the tokenizers for LLM A and B respectively. Taking into account the possible
existence of a begin-of-stream token and a possible begin-word-character symbol we deduce a map from $vocab_A$ and $vocab_B$ as follows:

   1. Since the RBO analytic as we apply it is not symmetric in its
   arguments (the $K$ and $p$ parameters are determined by $A$,
   **not** $B$) we're only interested in the map $f:vocab_B
   \rightarrow vocab_A$ so we can compare the proposed tokens from LLM
   B to those from LLM A. For each pair of ranked lists we're
   comparing, we're only interested in mapping the top $K$ entries
   from each list.

   2. For each entry in the list $A$ and each entry in the list $B$ if
   $tok_a$ is a prefix for $tok_b$ or vice versa, then we have a
   correspondence $tok_a \leftrightarrow tok_b$ It's possible that more
   than 1 token from $vocab_A$ might map to the same token from $vocab_B$
   and vice versa. If so we have a remediation strategy: we apply the
   Hungarian algorithm to solve a Minimum Cost Assignment problem.

We've provided a function, find_maximum_one_to_one_matching, in the submodule mapping to
implement our algorithm for finding a correspondence between token vocabularies *after* stripping
off the begin-new-word-symbol symbol from the tokens lists. In addition, we've also provided the helper functions
detect_begin_stream_token and detect_word_start_prefix.


The function detect_begin_stream_token helps to determine if the LLM's tokenizer always begins with a begin-stream-token or not. If it does, then the user will know to collect the logits for the second token generated (where the real response comes from) instead of the first token (which is just the "beginning of a stream" token).

The function detect_word_start_prefix helps to determine if a tokenizer uses a specialized prefix symbol to denote the beginning of a word. For example, compare 2 tokens from 2 different tokenizers:

> Ġhello vs. hello

If we didn't know that the first tokenizer uses a *Ġ* to denote the beginning of a word, we wouldn't be able to successfully programmatically detect that the 2 tokens are equivalent.

Here's sample code to demonstrate how we compare 2 LLMs.

```python

import numpy as np
import rbo_analytics
# We have N >> 1 list of prompts and 2 LLMs, llm_A and llm_B

# We want to know which llm might use a beginning-of-stream  token:

# vocab_a and vocab_b are the respective token vocabularies.

# lists_a is the list of tokenized prompts, i.e. list_a[n] is the
# sequence of prompts generated by LLM A's tokenizer for prompt n.

# lists_b is the same for LLM B's tokenizer

begin_a = rbo_analytics.mapping.detect_begin_stream_token(lists_a)
begin_b = rbo_analytics.mapping.detect_begin_stream_token(lists_b)

if begin_a is not None:
   print("Tokenizer A uses a begin-sequence token. Look at the logits for the second token")

if begin_b is not None:
   print("Tokenizer B uses a begin-sequence token. Look at the logits for the second token")

# detect if the tokenizers have a symbol prefix for the beginning of a word:
prefix_a = rbo_analytics.mapping.detect_word_start_prefix(vocab_a)
prefix_b = rbo_analytics.mapping.detect_word_start_prefix(vocab_b)

if prefix_a is not None:
   print("Tokenizer A uses a begin-of-word symbol. Modify the vocabulary.")
   vocab_a = [x[1:] if x.startswith(prefix_a) and len(x) > 1 else x for x in vocab_a]
if prefix_b is not None:
   print("Tokenizer B uses a begin-of-word symbol. Modify the vocabulary.")
   vocab_b = [x[1:] if x.startswith(prefix_b) and len(x) > 1 else x for x in vocab_b]

# Now generate responses from both LLMs for each of the prompts. Get
# the logits for the first token of each response.

# This is pseudo code. We only need at most 2 tokens if the first token is begin-of-stream token
# get_logits is a pseudo function that returns a list  of numpy arrays of logits, one array per token generated.
responses_A = [llmA(prompt).get_logits(:2)[1] if begin_a is not None else llmA(prompt).get_logits(0) \
for prompt in prompts]

responses_B = [llmB(prompt).get_logits(:2)[1] if begin_b is not None else llmB(prompt).get_logits(0) \
for prompt in prompts]

# Now that we have logits for the first token in the response for each of the prompts we
# need to order the tokens and their logits in descending order according to the logits.

sorted_responses_a = []
sorted_responses_b = []

for n in range(N): # the number of prompts is N
    sort_list_a = np.argsort(-responses_A[n]) # sort in descending probability
    sort_list_a = sort_list_a[:200] # I doubt that more than 200 tokens are going to matter.
    ranked_list_a = [vocab_a[t] for t in sort_list_a]
    probs_a = responses_A[n][sort_list_a]
    sorted_responses_a.append((ranked_list_a,probs_a))

    sort_list_b = np.argsort(-responses_B[n]) # sort in descending probability
    sort_list_b = sort_list_b[:200] # I doubt that more than 200 tokens are going to matter.
    ranked_list_b = [vocab_b[t] for t in sort_list_b]
    probs_b = responses_B[n][sort_list_b]
    sorted_responses_b.append((ranked_list_b,probs_b))

lists_a, probs_a = list(zip(*sorted_responses_a))
lists_b, probs_b = list(zip(*sorted_responses_b))

# Finally we can now compare:
Z = rbo_analytics.compute_recommender_test_statistic(lists_a, lists_b, probs_a,verbose=True)

print("Sigmage that the 2 LLMs are statistically related: {Z}")
print(f"The 2 LLMs are statistically related: {Z>=-2.33}")
```

## Strongly Comparing LLMs

Given that we can statistically measure the behavior of pairs of LLMs, one can also determine how
strongly they are related by constructing an $M \times M$ contingency table $ZScore[i,j]$ in which
we compute the $Z$ scores of all possible pairings $i,j$ for $M$ LLMs. Since our test statistic is not symmetric
in its arguments, the matrix $Zscore$ will not be symmetric.

Using a [biclustering algorithm](https://en.wikipedia.org/wiki/Biclustering)
we can group the $M$ LLMs into strongly related groups of LLMs.
