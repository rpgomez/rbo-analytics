{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814d31c8-3372-43bd-acbe-1883615a1a3e",
   "metadata": {},
   "source": [
    "## Comparing document embedding models\n",
    "\n",
    "* We have 2 different document embedding models: $A$ and $B$. \n",
    "* We have a corpus of $N \\gg 1$ documents. \n",
    "\n",
    "We would like to know if the 2 embedding models are statistically functionally related.\n",
    "\n",
    "### Solution: \n",
    "\n",
    "1. For each document $D_n$ compute its corresponding vector embedding $E_n := A(D_n)$, $F_n = B(D_n)$.\n",
    "2. For embedding model $A$ determine an appropriate value of $K$ as follows:\n",
    "   1. Randomly select  $M \\gg 1$ documents $\\{D_m\\}$ from the corpus. Let $\\{E_m\\}$ denote their corresponding vector embeddings.\n",
    "   2. For each vector $E_m$ find its $P \\ge 100$ closest neighbors $\\{E_{m,p}\\}$. **Exclude** $E_m$ from the list of neighbors. Compute the cosine similarity between vector $E_m$ and its $P$ closest neighors: $(<E_m,E_{m,p}>)$.\n",
    "   3. Sort the cosine similarities as an ascending sequence $(cos_p)$.\n",
    "   4. Find the elbow in the sequence of cosines $(cos_p)$ using an F-test to split the sequence into 2 subsequences. The size of the second subsequence is the corresponding value of $K_m$.\n",
    "   5. We can either take $K= \\max(\\{K_m\\})$ or keep the set of $K$ values $\\{K_m\\}$.\n",
    "3. Now that we have a value of $K$ for each document $D_m$ we have a corresponding persistence parameter $p=0.01^{(1/K)}$.\n",
    "4. For each document $D_m$ we have a ranked list of $K$ documents $list\\_a = (i_1,\\ldots,i_K)$ where $i_k$ denotes the index of the document that is $k^{th}$ closes to $D_m$   according to model $A$. We generate a similar second list of K ranked documents $list\\_b = (j_1,\\ldots, j_K)$ by proximity according to model $B$ (look at the corresponding vectors $(F_n)$.) For a corresponding probality distribution for model A's ranked list, we use the geometric based sequence \n",
    "$$prob = \\frac{(1-p)}{1-p^{K}}\\times (1,p,p^2, \\ldots, p_{K-1})$$\n",
    "5. Now we can use the rbo_analytics compute_recommender_test_statistic function to perform hypothesis testing.\n",
    "```python\n",
    "import rbo_analytics\n",
    "\n",
    "# lists_a is a list of ranked lists of neighbors for (D_m) according to recommender A.\n",
    "# lists_b is a list of ranked lists of neighbors for (D_m) according to recommender B.\n",
    "# probs is a list of list of probabilities, probs[m]  are the probabilities corresponding\n",
    "# to a list of ranked neighbors lists_a[m] for document m according to A. \n",
    "Z = rbo_analytics.compute_recommender_test_statistic(lists_a, lists_b,probs,verbose=True)\n",
    "\n",
    "\n",
    "print(\"Sigmage that the 2 document embedders are functionally related: {Z}\")\n",
    "print(f\"The 2 document embedders are functionally related: {Z>=-2.33}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3b5ae-f53f-4856-971f-177626098171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import rbo_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de360d8a-0611-4039-acc1-0b95cdab0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Define the device\n",
    "# It will automatically use 'cuda' if available, otherwise 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the model and explicitly specify the device\n",
    "model_name = 'intfloat/multilingual-e5-base'\n",
    "multilingual_e5_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# # Example Usage\n",
    "# sentences = [\"This is a test sentence.\", \"Dies ist ein Testsatz.\"]\n",
    "# embeddings = model.encode(sentences, show_progress_bar=True) \n",
    "\n",
    "# # To verify, you can inspect the model's internal device:\n",
    "# print(model.device) \n",
    "# # This should print \"cuda:0\" or similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9dafa3-1be0-4ba7-9b4c-ad87300458da",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mpnet_base_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "paraphrase_multilingual_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "multilingual_e5_model = SentenceTransformer(model_name, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8756dd95-5fee-4151-9009-9d1deca3823d",
   "metadata": {},
   "source": [
    "# Loading in the Usenet Newsgroup data set and embedding it\n",
    "I'm going to embed the corpus with 3 different document embedding algorithms so that I can measure the\n",
    "pairwise similarity between the 3 embedding algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cfdc9b-96c4-4cf6-bfd6-78a872797c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='all')\n",
    "\n",
    "data = dataset['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daaf0f4-af9f-4148-bdc7-9995a3ee01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_model(documents,model=all_mpnet_base_model):\n",
    "    \"\"\"Embed in batches of 4 \"\"\"\n",
    "    N = len(documents)\n",
    "    results = []\n",
    "    for t in tqdm(range(0,N,4),desc='embedding documents'):\n",
    "        input_texts = documents[t:t+4]\n",
    "        # Tokenize the input texts\n",
    "        numpified = model.encode(input_texts)\n",
    "        results.append(numpified)\n",
    "\n",
    "    embeddings_array  = np.vstack(results)\n",
    "    return embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fc3b4-9773-405d-bdf4-6231d4a8bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_e5_embeddings = encode_model(data,model=multilingual_e5_model)\n",
    "np.savetxt('embeddings_e5.numpy',multilingual_e5_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f9265-4367-4f6f-9020-b8c8e670349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = encode_model(data)\n",
    "np.savetxt('all-mpnet.numpy',embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c506e8-d422-4403-8778-96c5d99886ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = encode_model(data,model=paraphrase_multilingual_model)\n",
    "np.savetxt('paraphrase-multilingual.numpy',embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef95a41-c01d-4c8b-ac50-38fc21e5913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_multilingual = np.loadtxt('paraphrase-multilingual.numpy')\n",
    "all_mpnet = np.loadtxt('all-mpnet.numpy')\n",
    "multilingual_e5 = np.loadtxt('embeddings_e5.numpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47caf74e-b015-4209-9d2f-c2d0eaab0de2",
   "metadata": {},
   "source": [
    "# 3 Way Comparison of all-mpnet-base-v2, paraphrase-multilingual-MiniLM-L12-v2, and multilingual-e5-base\n",
    "Here is where we will construct lists of rankings for randomly selected documents in the corpus of newsgroup postings\n",
    "and then compute the $Z$ statistic as to whether or not pairs of document embedders are functionally related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edddbbf-89f8-4def-8394-6126367d3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (paraphrase_multilingual**2).sum(axis=1).reshape(-1,1)\n",
    "distances_paraphrase= X + X.T - 2*paraphrase_multilingual.dot(paraphrase_multilingual.T)\n",
    "distances_paraphrase[distances_paraphrase<0.0] = 0.0\n",
    "distances_paraphrase =distances_paraphrase**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ec829-70d2-4089-90a6-343516a1049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (all_mpnet**2).sum(axis=1).reshape(-1,1)\n",
    "distances_all_mpnet= Y + Y.T - 2*all_mpnet.dot(all_mpnet.T)\n",
    "distances_all_mpnet[distances_all_mpnet<0.0] = 0.0\n",
    "distances_all_mpnet =distances_all_mpnet**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed83b2-557b-433c-b4ac-a64b237e8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = (multilingual_e5**2).sum(axis=1).reshape(-1,1)\n",
    "distances_multilingual_e5= Y + Y.T - 2*multilingual_e5.dot(multilingual_e5.T)\n",
    "distances_multilingual_e5[distances_multilingual_e5<0.0] = 0.0\n",
    "distances_multilingual_e5 =distances_multilingual_e5**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1671f-38ed-48fb-923d-e354ce61d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_geometric_probs(K):\n",
    "    \"\"\"Generates a probability distribution, Pr(x = n) ~ p**n \"\"\"\n",
    "\n",
    "    p = (0.01)**(1/K)\n",
    "    probs = p**np.arange(K)\n",
    "    probs = probs/probs.sum()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113debd6-4c36-47a0-8835-7bd42bff8dfd",
   "metadata": {},
   "source": [
    "I'm going to randomly select 100 postings and for each of the postings finding the $K$ nearest neighbors according to the document embedders.\n",
    "I'm going to use $K=30$ for each neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c5f88-9b4c-44ea-9879-3689fc825d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "np.random.seed(42) # For reproducible results.\n",
    "ks = np.random.choice(N,size=100,replace=False) # random sampling of documents\n",
    "Ks = np.ones(100,dtype=np.int32)*30 # how many neighbors per document to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04392c07-2909-4611-b39c-5857713e61ee",
   "metadata": {},
   "source": [
    "# Comparing paraphrase-multilingual-MiniLM-L12-v2 against all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed8389-19a0-42bb-8aba-c99782a8990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_a = [((distances_paraphrase[n]).argsort()[1:Ks[t]]).tolist() for t,n in enumerate(ks)]\n",
    "lists_b = [((distances_all_mpnet[n]).argsort()[1:Ks[t]]).tolist() for t,n in enumerate(ks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4525c7-b51d-4c81-8a38-d9fd2edfafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_a  = [generate_geometric_probs(K-1) for K in Ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2a0be-fc3e-4f36-ae5c-12c24856e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = rbo_analytics.compute_recommender_test_statistic(lists_a, lists_b,probs_a,verbose=True)\n",
    "\n",
    "print(f\"Sigmage that the 2 Document embedders are functionally related: {Z}\")\n",
    "\n",
    "if Z >= -2.33:\n",
    "    print(\"The 2 Document embedding algorithms are functionally related.\")\n",
    "else:\n",
    "    print(\"The 2 Document embedding algorithms are *NOT* functionally related.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a6f40-5864-459c-a9f3-ee6f07e1f247",
   "metadata": {},
   "source": [
    "I want to reverse the order of the lists to demonstrate that the $Z$ statistic is not symmetric in its arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd8906-9dfe-4716-b7f0-b49bef30174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = rbo_analytics.compute_recommender_test_statistic(lists_b, lists_a,probs_a,verbose=True)\n",
    "\n",
    "print(f\"Sigmage that the 2 Document embedders are functionally related: {Z}\")\n",
    "\n",
    "if Z >= -2.33:\n",
    "    print(\"The 2 Document embedding algorithms are functionally related.\")\n",
    "else:\n",
    "    print(\"The 2 Document embedding algorithms are *NOT* functionally related.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be67b00-6005-4e75-8088-d4f0998fd726",
   "metadata": {},
   "source": [
    "# Comparing paraphrase-multilingual-MiniLM-L12-v2 against multilingual-e5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d6185-19e9-4e4e-be73-b3df8ddd4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_c = [((distances_multilingual_e5[n]).argsort()[1:Ks[t]]).tolist() for t,n in enumerate(ks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336dad4-2562-41cf-933b-0d02b36b448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = rbo_analytics.compute_recommender_test_statistic(lists_a, lists_c,probs_a,verbose=True)\n",
    "\n",
    "print(f\"Sigmage that the 2 Document embedders are functionally related: {Z}\")\n",
    "\n",
    "if Z >= -2.33:\n",
    "    print(\"The 2 Document embedding algorithms are functionally related.\")\n",
    "else:\n",
    "    print(\"The 2 Document embedding algorithms are *NOT* functionally related.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d46c21-0438-4c9e-824b-dff13ef8b908",
   "metadata": {},
   "source": [
    "## Comparing all-mpnet-base-v2 against multilingual-e5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0ffbb-7b95-468f-aab5-f43b543f4f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = rbo_analytics.compute_recommender_test_statistic(lists_b, lists_c,probs_a,verbose=True)\n",
    "\n",
    "print(f\"Sigmage that the 2 Document embedders are functionally related: {Z}\")\n",
    "\n",
    "if Z >= -2.33:\n",
    "    print(\"The 2 Document embedding algorithms are functionally related.\")\n",
    "else:\n",
    "    print(\"The 2 Document embedding algorithms are *NOT* functionally related.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81862402-8e82-4e55-abf1-f0dd20c2a5b7",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Based on $Z$ statistics if I were to construct a graph of similarity, where edge weights are the $Z$ statistic I would conclude that\n",
    "the all-mpnet-base-v2 algorithm is strongly similar in embedding documents with the multilingual-e5-base algorithm ($Z \\approx 13.3$), \n",
    "that paraphrase-multilingual-MiniLM-L12-v2 and multilingual-e5-base are strongly related ($Z \\approx 4.4$) and that \n",
    "all-mpnet-base-v2 is also strongly related to paraphrase-multilingual-MiniLM-L12-v2 ($Z \\approx 4.7$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
